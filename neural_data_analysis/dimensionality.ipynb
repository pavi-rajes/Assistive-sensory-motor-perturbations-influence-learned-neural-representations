{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import copy\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions \n",
    "###### helper functions ####### \n",
    "def pkl_read(file_to_read, write_dir):\n",
    "  this = pickle.load(open(os.path.join(write_dir,file_to_read), \"rb\"))\n",
    "  # print(this)\n",
    "  return this\n",
    "\n",
    "def pkl_write(file_to_write, values_to_dump, write_dir):\n",
    "  os.chdir(write_dir)\n",
    "  with open(os.path.join(write_dir,file_to_write), 'wb') as pickle_file:\n",
    "      pickle.dump(values_to_dump, pickle_file)\n",
    "\n",
    "def getdata_day(data,id,data_day):\n",
    "  if id == 1:\n",
    "    # var_name = data[0:data_day[id]+1,:,:]\n",
    "    var_name = data[0:data_day[id],:,:]\n",
    "  else:\n",
    "    day_idx = np.where(np.array(list(data_day))== id)[0].astype(int) - 1\n",
    "    #print(day_idx[0])\n",
    "    prev_day_id = list(data_day)[day_idx[0]]\n",
    "    #print(prev_day_id)\n",
    "    var_name = data[data_day[prev_day_id]:data_day[id],:,:]\n",
    "  return var_name\n",
    "\n",
    "def gettarget_day(target_direction, id, day):\n",
    "    if id == 1: \n",
    "      target = target_direction[:day[id]]\n",
    "    else:\n",
    "      day_idx = np.where(np.array(list(day))== id)[0].astype(int) - 1\n",
    "      #print(day_idx[0])\n",
    "      prev_day_id = list(day)[day_idx[0]]\n",
    "      target = target_direction[day[prev_day_id]:day[id]] \n",
    "    return target\n",
    "\n",
    "def get_numtrials_perday(datasize_day):\n",
    "    '''\n",
    "    datasize_day: dictionary with key as day and value as end trial index for that day . Comes from getdata_day()\n",
    "    '''\n",
    "    num_trials_perday = np.zeros(len(datasize_day))\n",
    "    num_trials_perday[0] = np.array(list(datasize_day.values()))[0]\n",
    "    num_trials_perday[1:] = np.diff(np.array(list(datasize_day.values())))\n",
    "    # print(num_trials_perday)\n",
    "    return num_trials_perday\n",
    "\n",
    "def trial_concatenate_data(data):\n",
    "    '''\n",
    "    data is on shape n_trials x n_units x n_bins \n",
    "    '''\n",
    "    n,u,t = data.shape\n",
    "    new_arr = np.transpose(data, (0,2,1))\n",
    "    new_arr = new_arr.reshape(n*t, u)\n",
    "    # print(new_arr.shape)\n",
    "    return new_arr \n",
    "\n",
    "###### PR analysis functions #########\n",
    "\n",
    "def calculate_PR(data, normalize_pr = True, normalize_data = 'zscore', sqrt_transform = False ):\n",
    "    '''\n",
    "    Args: \n",
    "    data (2D Numpy array): Neural data in format (n_timepoints, n_units)\n",
    "    '''\n",
    "    # n_t, n_u = data.shape\n",
    "    if  normalize_data == 'zscore': \n",
    "        scaler_ = StandardScaler()\n",
    "        standardized_data = scaler_.fit_transform(data)\n",
    "        data = standardized_data\n",
    "    if normalize_data  == 'demean':\n",
    "        scaler_ = StandardScaler(with_mean = True, with_std = False)\n",
    "        standardized_data = scaler_.fit_transform(data)\n",
    "        data = standardized_data\n",
    "#         print(scaler_.mean_)\n",
    "        \n",
    "    if sqrt_transform:\n",
    "        data = np.sqrt(data + 0.375)  # see Kihlberg, 1972; 0.386 could also be a good value\n",
    "            \n",
    "    C_unit = np.cov(data.T)\n",
    "#     print(C_unit.shape)\n",
    "    eig_val, eig_vec = np.linalg.eig(C_unit)\n",
    "    PR = (np.sum(eig_val))**2/ np.sum(eig_val**2)\n",
    "    return (PR - 1) / (data.shape[1] - 1) if normalize_pr else PR\n",
    "\n",
    "def hypothesis_testing(data1, data2, sqrt_transform=False, normalize_data = 'zscore',\n",
    "                       nb_permutations=int(1e4), use_normalized_PR=False):\n",
    "    \"\"\"\n",
    "    Null hypothesis : no difference in the mean PR between the two datasets `data1` and `data2`\n",
    "    Alternative : PR(data2) > PR(data1)  (direction is important).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data1 : 2D np.ndarray of integers with shape (n_time_bins, n_units)\n",
    "        Design matrix for first dataset. Each row contains the spike counts of the units in the corresponding time bin.\n",
    "    data2: 2D np.ndarray of integers with shape (n_time_bins, n_units)\n",
    "        Design matrix for second dataset.\n",
    "    sqrt_transform : bool (default: False)\n",
    "        Whether to transform the count data before computing PR_norm.\n",
    "    nb_perturbations : int\n",
    "        Number of permutations of day index.\n",
    "    use_normalized_PR : bool (default: False)\n",
    "        Whether to compute the normalized PR or the participation ratio itself\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    p-value : float\n",
    "    \"\"\"\n",
    "    n_samples1, n_units1 = data1.shape\n",
    "    n_samples2, n_units2 = data2.shape\n",
    "    assert n_units1 == n_units2, \"`data1` and `data2` must have the same number of units\"\n",
    "    \n",
    "    late_pr =  calculate_PR(data2, normalize_pr= use_normalized_PR,normalize_data=normalize_data, sqrt_transform=sqrt_transform)\n",
    "    print('late_pr: ',late_pr)\n",
    "    early_pr =  calculate_PR(data1,  normalize_pr= use_normalized_PR,normalize_data=normalize_data, sqrt_transform=sqrt_transform)\n",
    "    print('early_pr: ',early_pr)\n",
    "    test_statistic = late_pr - early_pr # direction is important\n",
    "    print('delta_pr: ',test_statistic)\n",
    "\n",
    "    # Compute bootstrapped delta_PR estimates\n",
    "    bootstrapped_delta_PRs = np.empty(shape=nb_permutations)\n",
    "\n",
    "    null_PRs = np.empty(shape=nb_permutations)\n",
    "    data_cat = np.vstack((data1, data2))\n",
    "\n",
    "    for i in range(nb_permutations):\n",
    "        # true delta PR distribution\n",
    "        bootstrap_data1 = resample(data1)\n",
    "        bootstrap_data2 = resample(data2)\n",
    "        bootstrapped_pr1 = calculate_PR(bootstrap_data1, normalize_pr=use_normalized_PR,\n",
    "                                        normalize_data=normalize_data, sqrt_transform=sqrt_transform)\n",
    "        bootstrapped_pr2 = calculate_PR(bootstrap_data2, normalize_pr=use_normalized_PR,\n",
    "                                        normalize_data=normalize_data, sqrt_transform=sqrt_transform)\n",
    "        bootstrapped_delta_PRs[i] = bootstrapped_pr2 - bootstrapped_pr1\n",
    "\n",
    "        # Null delta PR distribution\n",
    "        data_cat_shuffle = shuffle(data_cat)\n",
    "        data1_permut, data2_permut = data_cat_shuffle[:n_samples1], data_cat_shuffle[n_samples1:]\n",
    "        null_PRs[i] = calculate_PR(data2_permut, normalize_pr= use_normalized_PR,normalize_data=normalize_data, sqrt_transform=sqrt_transform) - calculate_PR(data1_permut, normalize_pr= use_normalized_PR,normalize_data=normalize_data, sqrt_transform=sqrt_transform)\n",
    "\n",
    "    # P-value\n",
    "    p_value = np.sum(null_PRs > test_statistic) / nb_permutations\n",
    "  \n",
    "    return p_value, test_statistic, null_PRs, bootstrapped_delta_PRs\n",
    "\n",
    "###### Statistics functions ######## \n",
    "\n",
    "def compute_ci(data, alpha = 0.95):\n",
    "    import scipy.stats as st\n",
    "    ci = st.t.interval( df=len(data)-1, loc=np.mean(data), scale=st.sem(data), confidence=alpha)\n",
    "    return ci # returns actual values of lower and upper CI \n",
    "\n",
    "def compute_moe(data, alpha = 0.95):\n",
    "    import scipy.stats as st\n",
    "    moe = st.t.ppf((1 + alpha) / 2., len(data)-1) * st.sem(data) \n",
    "    return moe # returns margin of error for lower and upper CI - can be used for errorbars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pkl_read' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neural_recordings \u001b[39m=\u001b[39m pkl_read(\u001b[39m'\u001b[39m\u001b[39mneural_data.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m./example_data/\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Shape: n_tr x n_units x n_timebins; top 16 units are readouts \u001b[39;00m\n\u001b[1;32m      2\u001b[0m target_direction \u001b[39m=\u001b[39m pkl_read(\u001b[39m'\u001b[39m\u001b[39mtarget_labels.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m./example_data/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m datasize_day \u001b[39m=\u001b[39m pkl_read(\u001b[39m'\u001b[39m\u001b[39mtrial_day_label.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m./example_data/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pkl_read' is not defined"
     ]
    }
   ],
   "source": [
    "neural_recordings = pkl_read('neural_data.pkl', './example_data/') # Shape: n_tr x n_units x n_timebins; top 16 units are readouts \n",
    "target_direction = pkl_read('target_labels.pkl', './example_data/')\n",
    "datasize_day = pkl_read('trial_day_label.pkl', './example_data/')\n",
    "\n",
    "days = [2,3,4,5,6] # just for example. In the paper, we considered all days that had a minimum of 25 trials of successful reaches per target direction, minimum 200 trials in total.  \n",
    "n_readouts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error bars for Global PR from 10000 draws on all trials\n",
    "n_bs = 10 #10000 \n",
    "normalize_data = 'demean'\n",
    "\n",
    "g_normPR_all_bs = np.zeros((n_bs, len(days)))\n",
    "g_normPR_r_bs = np.zeros((n_bs, len(days)))\n",
    "g_normPR_nr_bs = np.zeros((n_bs, len(days)))\n",
    "\n",
    "# l_normPR_all_bs = np.zeros((n_bs, len(days), 8))\n",
    "# l_normPR_r_bs = np.zeros((n_bs, len(days), 8))\n",
    "# l_normPR_nr_bs = np.zeros((n_bs, len(days), 8))\n",
    "\n",
    "\n",
    "for idraw in tqdm(range(n_bs)): \n",
    "\n",
    "      for id, iday in tqdm(enumerate(days)): \n",
    "            # per day\n",
    "            data_all = getdata_day(neural_recordings,iday, datasize_day )\n",
    "\n",
    "\n",
    "            data = trial_concatenate_data(data_all)\n",
    "            # print(data.shape)\n",
    "\n",
    "            g_normPR_all_bs[idraw, id] = calculate_PR(data, normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "            g_normPR_r_bs[idraw, id] = calculate_PR(data[:, :n_readouts], normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "            g_normPR_nr_bs[idraw, id] = calculate_PR(data[:,n_readouts:], normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "\n",
    "            ## this is to compute PR per target direction\n",
    "            # target = gettarget_day(target_direction, iday, datasize_day)\n",
    "            # print(np.unique(target))\n",
    "            # for it in np.unique(target).astype(int):\n",
    "            #       it_idx = np.where(target[idx] == it)[0]\n",
    "            #       data_it = data_idx[it_idx,:,:]\n",
    "            #       data_it = trial_concatenate_data(data_it)\n",
    "            #       # print(data.shape, it)\n",
    "\n",
    "            #       l_normPR_all_bs[idraw, id, it-1] = calculate_PR(data_it, normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "            #       l_normPR_r_bs[idraw, id, it-1] = calculate_PR(data_it[:, :direct_idx_2.shape[0]], normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "            #       l_normPR_nr_bs[idraw, id, it-1] = calculate_PR(data_it[:, direct_idx_2.shape[0]:], normalize_pr= True, normalize_data=normalize_data, sqrt_transform=False)\n",
    "\n",
    "# Compute statistics from bootstrap\n",
    "ci_all = np.zeros((g_normPR_all_bs.shape[1],2))\n",
    "ci_r = np.zeros((g_normPR_r_bs.shape[1],2))\n",
    "ci_nr = np.zeros((g_normPR_nr_bs.shape[1],2))\n",
    "\n",
    "for i in range(g_normPR_r_bs.shape[1]): \n",
    "    ci_all[i,:] = compute_moe(g_normPR_all_bs[:,i])\n",
    "    ci_r[i,:] = compute_moe(g_normPR_r_bs[:,i])\n",
    "    ci_nr[i,:] = compute_moe(g_normPR_nr_bs[:,i])\n",
    "\n",
    "# Save results in a dataframe\n",
    "df_prnorm = pd.DataFrame() \n",
    "df_prnorm['days'] = days\n",
    "df_prnorm['pr_all_mean'] = np.mean(g_normPR_all_bs, axis = 0)\n",
    "df_prnorm['pr_r_mean'] = np.mean(g_normPR_r_bs, axis = 0)\n",
    "df_prnorm['pr_nr_mean'] = np.mean(g_normPR_nr_bs, axis = 0)\n",
    "\n",
    "df_prnorm['pr_all_ci_l'] =  ci_all[:,0] # this is the actual lower bound CI PR value\n",
    "df_prnorm['pr_r_ci_l'] = ci_r[:,0]\n",
    "df_prnorm['pr_nr_ci_l'] =  ci_nr[:,0]\n",
    "\n",
    "df_prnorm['pr_all_ci_u'] = ci_all[:,1] \n",
    "df_prnorm['pr_r_ci_u'] = ci_r[:,1] \n",
    "df_prnorm['pr_nr_ci_u'] = ci_nr[:,1] \n",
    "\n",
    "# plot results - Similar to Figure 2A, 2B in Paper \n",
    "sns.set_context('paper')\n",
    "plt.figure(figsize = (2,1.5))\n",
    "plt.errorbar(np.array(df_prnorm['days']),df_prnorm['pr_r_mean'], yerr = np.vstack((np.array(df_prnorm['pr_r_ci_l']), np.array(df_prnorm['pr_r_ci_u']))), color = 'r', label = 'readout', marker = 'o')\n",
    "plt.errorbar(np.array(df_prnorm['days']),df_prnorm['pr_nr_mean'], yerr = np.vstack((np.array(df_prnorm['pr_nr_ci_l']), np.array(df_prnorm['pr_nr_ci_u']))), color = 'b',  label = 'nonreadout', marker = 'o')\n",
    "# plt.plot(np.array(df_prnorm['days']),df_prnorm['pr_r_mean'],color = 'r', label = 'readout')\n",
    "# plt.plot(np.array(df_prnorm['days']),df_prnorm['pr_nr_mean'],color = 'b',  label = 'nonreadout')\n",
    "# plt.fill_between(np.array(df_prnorm['days']),df_prnorm['pr_r_ci_l'], df_prnorm['pr_r_ci_u'], color = 'r', alpha = 0.4)\n",
    "# plt.fill_between(np.array(df_prnorm['days']),df_prnorm['pr_nr_ci_l'], df_prnorm['pr_nr_ci_u'], color = 'b', alpha = 0.4)\n",
    "\n",
    "plt.legend( frameon = False, labelcolor = 'linecolor', fontsize = 7)\n",
    "plt.xticks(days, fontsize = 7)\n",
    "plt.yticks(fontsize = 7)\n",
    "# plt.title('PCA dimensionality')\n",
    "plt.xlabel('Days', fontsize = 7)\n",
    "plt.ylabel('$PR_{norm}$', fontsize = 7)\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing delta_PR distributions \n",
    "early_day = days[1]\n",
    "late_day = days[-1]\n",
    " \n",
    "early_data_trials = getdata_day(neural_recordings, early_day, datasize_day)\n",
    "early_data_trials = np.squeeze(early_data_trials[:,:n_readouts,:])\n",
    "early_data = trial_concatenate_data(early_data_trials)\n",
    "\n",
    "late_data_trials = getdata_day(neural_recordings, late_day, datasize_day)\n",
    "late_data_trials = np.squeeze(late_data_trials[:,:n_readouts,:]) \n",
    "late_data = trial_concatenate_data(late_data_trials)\n",
    "\n",
    "p, test_stat,  null_prs, delta_prs = hypothesis_testing(late_data, early_data, nb_permutations=10000, normalize_data= 'zscore', use_normalized_PR = True)\n",
    "# delta_prs is the distribution of delta_pr obtained through bootstrapping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
